{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gk/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "\n",
    "Rectified Linear Unit (ReLU) is a goto activation function for hidden layers in present days replacing sigmoid function which was generally. It replaces negative activations with zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z_values):\n",
    "    return np.maximum(0,Z_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MLP():\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, Y, Layers, Weights=[], Biases=[]):\n",
    "    # Setting weights and biases for random values if unspecified\n",
    "    if len(Weights) == 0:\n",
    "        Weights.append(np.random.rand(X.shape[1],Layers[0]))\n",
    "        for i in range(1,len(Layers)):\n",
    "            Weights.append(np.random.rand(Weights[i-1].shape[1],Layers[i]))\n",
    "    if len(Biases) == 0:\n",
    "        Biases = [np.random.rand(n_units) for n_units in Layers]\n",
    "    \n",
    "    Z_values = []\n",
    "    Activations = []\n",
    "#     for i in Weights:\n",
    "#         print(i.shape)\n",
    "    # Setting up the 1st layer\n",
    "    dot_product = np.dot(X, Weights[0]) \n",
    "    L1_value = dot_product + Biases[0][np.newaxis, :]\n",
    "    Z_values.append(L1_value)\n",
    "    Activations.append(ReLU(L1_value))  # Using L1_value here\n",
    "    \n",
    "    # Iterating for the remaining layers\n",
    "    for i in range(len(Layers)-1):  \n",
    "        dot_product = np.dot(Activations[i], Weights[i+1]) # Start from 1, since the first layer is already handled\n",
    "        Z_value=dot_product+ Biases[i+1]\n",
    "        Z_values.append(Z_value)\n",
    "        Activations.append(ReLU(Z_value))\n",
    "\n",
    "    \n",
    "    return  Activations, Weights, Biases,Z_values,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \t [[1.06920067 2.104715   1.55729685 1.85038308]\n",
      " [0.94095158 1.91144357 1.55090212 1.93308128]\n",
      " [0.90579863 1.54458702 1.13694114 1.51443638]\n",
      " [0.57012035 1.68949169 0.75868088 1.2674275 ]\n",
      " [0.61058223 1.65121325 1.30467679 1.86609261]\n",
      " [0.70111859 1.28620763 1.14530171 1.66333103]\n",
      " [0.81576991 1.75855305 0.93369633 1.31836098]\n",
      " [1.01736137 1.82775441 1.46247471 1.79567234]\n",
      " [0.8740164  1.36021535 1.10686242 1.51389556]\n",
      " [0.77499237 1.79739762 1.2142509  1.65667514]] \t Shape:(10, 4) \n",
      "\n",
      "10 \t [[3.74292078 3.64223857]\n",
      " [3.56905214 3.68306048]\n",
      " [2.89890235 2.85573077]\n",
      " [2.70881391 2.26562605]\n",
      " [3.13550629 3.33798821]\n",
      " [2.68092213 2.96014108]\n",
      " [2.9154098  2.50088312]\n",
      " [3.42073284 3.46576745]\n",
      " [2.71227371 2.80311712]\n",
      " [3.18627167 3.08393476]] \t Shape:(10, 2) \n",
      "\n",
      "10 \t [[4.69392806]\n",
      " [4.71491863]\n",
      " [3.81938592]\n",
      " [3.2129075 ]\n",
      " [4.32413309]\n",
      " [3.89848843]\n",
      " [3.46935593]\n",
      " [4.48281661]\n",
      " [3.74629189]\n",
      " [4.07789463]] \t Shape:(10, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "value,_,_,_=forward_propagation(np.random.rand(10,3),[1,2,3,4,5,6,7,8,9,10],[4,2,1])\n",
    "for i in value:\n",
    "    print(f\"{len(i)} \\t {i} \\t Shape:{i.shape} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(Y,calculated_value):\n",
    "    difference=(np.array(Y)-calculated_value)**2\n",
    "    return np.sum(difference)/len(Y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(Weights,Biases,d_cost_by_activation,d_activation_by_Z):    \n",
    "   \n",
    "    d_Z_by_weight=Weights\n",
    "    d_cost_by_weight=d_cost_by_activation * d_activation_by_Z*d_Z_by_weight\n",
    "    d_Z_by_bias=1 \n",
    "    d_cost_by_bias=np.sum(d_cost_by_activation * d_activation_by_Z*d_Z_by_bias)\n",
    "    return d_cost_by_weight,d_cost_by_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def backward_propagation(X,Y,W,b,Activations,learning_rate):\n",
    "    d_cost_by_activation=np.sum(Activations[-1]-np.array(Y).reshape(-1, 1))\n",
    "#     print(d_cost_by_activation,'\\n',Activations[-1],'\\n',np.array(Y))\n",
    "    d_activation_by_Z=1\n",
    "    new_activation=Activations.insert(0,X)\n",
    "    new_weights=[]\n",
    "    new_biases=[]\n",
    "    for i in range(len(W)-1,0,-1):\n",
    "        calc_weight,calc_bias=calculate_gradients(new_activation,b[i],d_cost_by_activation,d_activation_by_Z)\n",
    "        print(calc_weight,\"W[i]\\n\",W[i])\n",
    "        n_layer_wt=W[i]-learning_rate*calc_weight[i]\n",
    "        n_layer_b=b[i]-learning_rate*calc_bias[i]\n",
    "        new_weights.append(n_layer_wt)\n",
    "        new_biases.append(n_layer_b)\n",
    "    return new_weights,new_biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MLP(X,y,Layers,iterations,learning_rate,error_margin):\n",
    "    iteration_count=0\n",
    "    cost=100\n",
    "    cost_history=[]\n",
    "    W=[]\n",
    "    B=[]\n",
    "    while iteration_count<iterations or cost<=error_margin :\n",
    "        print(f\"Iteration: {iteration_count+1}\")\n",
    "        activation,weight,bias,_=forward_propagation(X,y,Layers,W,B)\n",
    "        cost=calculate_cost(y,activation[-1])\n",
    "        cost_history.append(cost)\n",
    "        W,B=backward_propagation(X,y,weight,bias,activation,learning_rate)\n",
    "        iteration_count+=1\n",
    "    return weights,biases,layers,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e4a32799c84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-f042468af8fb>\u001b[0m in \u001b[0;36mMLP\u001b[0;34m(X, y, Layers, iterations, learning_rate, error_margin)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcost_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0miteration_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cefc1754ea25>\u001b[0m in \u001b[0;36mbackward_propagation\u001b[0;34m(X, Y, W, b, Activations, learning_rate)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnew_biases\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcalc_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcalc_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_cost_by_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_activation_by_Z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"W[i]\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mn_layer_wt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcalc_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-eec61f4f8c17>\u001b[0m in \u001b[0;36mcalculate_gradients\u001b[0;34m(Weights, Biases, d_cost_by_activation, d_activation_by_Z)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0md_Z_by_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWeights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0md_cost_by_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_cost_by_activation\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_activation_by_Z\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_Z_by_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0md_Z_by_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0md_cost_by_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_cost_by_activation\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_activation_by_Z\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md_Z_by_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "wt,bias,layers,cost_history=MLP(np.random.rand(10,3),[1,2,3,4,5,6,7,8,9,10],[4,2,1],100,0.05,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[1,2,3]\n",
    "b=[4,5,6]\n",
    "\n",
    "np.array(b)-np.array(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:\"a\",2:\"B\"}\n",
    "a.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1():\n",
    "    def func2():\n",
    "        print(\"konichiwas broskis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.insert(0,5)\n",
    "aa[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4,5]\n",
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
